{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPOZ9bht3vu40tAjcE4k9Op",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hgabrali/NLP-Foundations-to-Frontiers/blob/main/Natural_Language_Processing_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Natural Language Processing: From Unstructured Data to Strategic Intelligence\n",
        "\n",
        "## 1. Executive Summary\n",
        "Natural Language Processing (NLP) serves as the critical bridge between the \"noisy\" reality of unstructured data‚Äîcomprising emails, social media, and clinical notes‚Äîand the strategic requirements of actionable business intelligence **(AWS)**. In an era where organizations generate massive volumes of text and voice data, NLP allows machines to interpret, manipulate, and comprehend human language to unlock significant competitive advantages **(AWS)**.\n",
        "\n",
        "The core value proposition of modern NLP lies in its ability to transform raw text into structured knowledge through advanced preprocessing pipelines and **Graph-based Retrieval-Augmented Generation (GraphRAG)**, which enables multi-hop reasoning across complex document sets **(arXiv)**. For enterprise-scale applications, particularly within complex **Enterprise Resource Planning (ERP)** environments like **SAP S/4HANA**, GraphRAG has become essential for reasoning over configuration rules and transactional dependencies **(arXiv)**.\n",
        "\n",
        "However, a significant strategic tension exists regarding efficiency vs. accuracy. In high-stakes sectors like healthcare and finance, the high computational costs of **Large Language Model (LLM)** based knowledge graph construction are often prohibitive. Research indicates that \"industrial-grade\" alternatives, such as **dependency-based knowledge graph construction**, can achieve **94%** of the performance of LLM-driven systems while remaining scalable and cost-effective **(arXiv)**. To reach this industrial-grade performance, organizations must first master the foundational technical stages required to prepare text for analysis.\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "## 2. Foundations of the NLP Preprocessing Pipeline\n",
        "Text preprocessing is not merely a \"cleanup\" task; it is a strategic necessity for improving model performance and managing vocabulary size across diverse datasets **(Scale Events, Meegle)**. By transforming raw, noisy text into a consistent format, organizations ensure that downstream models are fed structured inputs that minimize semantic interference.\n",
        "\n",
        "### üîç Evaluation of Essential Stages\n",
        "* **Segmentation**: This stage partitions text into individual sentences. The primary technical challenge involves the ambiguity of punctuation; for instance, a period marks a sentence boundary but also appears in abbreviations like \"Inc.\" **(Scale Events)**. Failure here leads to \"fragmented data\" that disrupts context; sentence-level units are far more amenable to syntactic parsing and enhance the performance of LLMs **(arXiv)**.\n",
        "* **Tokenization**: Sentences are converted into \"tokens\" (individual words or phrases). This is the building block for all subsequent analysis, yet standard whitespace splitting often fails with contractions like \"don't,\" which require specialized rules to preserve meaning **(Scale Events)**.\n",
        "* **Case Normalization**: Most NLP software chooses to lowercase all text to ensure consistency. This prevents a model from treating \"Apple\" (the brand) and \"apple\" (the fruit) as distinct entities solely due to capitalization **(Scale Events)**.\n",
        "* **Spell Correction**: This step prevents typographical errors from diluting the vocabulary, which is essential for maintaining high recall in classification and retrieval tasks **(Scale Events)**.\n",
        "\n",
        "### üõ°Ô∏è The \"Noise\" Reduction Layer\n",
        "**Stop-word removal** involves filtering out frequently occurring words like \"the,\" \"is,\" and \"of\" that provide little discriminatory value for tasks like document categorization or sentiment analysis **(Scale Events, ResearchGate)**. Research indicates that removing these words reduces the feature space and enhances computational efficiency.\n",
        "\n",
        "* **Arabic Context**: This category is expanded to include specific pronouns, days of the week, and months **(ResearchGate)**.\n",
        "* **Low-Resource Languages**: Specific libraries have been developed, such as **LiHiSTO** for Hindi (containing 820 stop-words) and **LiSTOM** for Malayalam, to improve retrieval performance **(ResearchGate)**.\n",
        "\n",
        "### üìä Comparative Analysis: Stemming vs. Lemmatization\n",
        "While both techniques reduce words to a base form, lemmatization offers a more sophisticated, context-aware approach.\n",
        "\n",
        "| Feature | Stemming (Crude Heuristic) | Lemmatization (Morphological Analysis) |\n",
        "| :--- | :--- | :--- |\n",
        "| **Goal** | Chop off affixes to find a base \"stem\" **(Scale Events)**. | Identify the linguistically valid \"lemma\" or dictionary form **(Stack Overflow)**. |\n",
        "| **Accuracy** | Lower; may produce non-words (e.g., \"caring\" becomes \"car\") **(Stack Overflow)**. | Higher; ensures a valid root (e.g., \"caring\" becomes \"care\") **(Stack Overflow)**. |\n",
        "| **Computational Cost** | Low; faster as it uses simple rules or lookup tables **(Scale Events)**. | High; slower as it requires morphological analysis and dictionaries **(Scale Events)**. |\n",
        "| **POS Awareness** | No; operates on individual words without context **(Stack Overflow)**. | Yes; uses Parts of Speech (POS) to determine meaning **(Stack Overflow)**. |\n",
        "\n",
        "> **The \"So What?\" of Context Awareness**: Lemmatization is vital when word meaning depends on usage. For example, a lemmatizer can distinguish between \"dove\" as a noun (the bird) and \"dove\" as a verb (past tense of dive), whereas a stemmer would treat them identically. Similarly, it can identify that \"better\" has \"good\" as its lemma **(Stack Overflow)**.\n",
        "\n",
        "---\n",
        "\n",
        "## 3. Structural Analysis: Parsing and Information Extraction\n",
        "Strategic NLP requires moving beyond \"Bag of Words\" approaches to understand grammatical relationships. This level of structural analysis allows systems to identify precise interactions between entities within a sentence **(arXiv)**.\n",
        "\n",
        "### üèóÔ∏è Contrast Parsing Methodologies\n",
        "* **Constituency Parsing**: Breaks text into sub-phrases or hierarchical segments (Noun Phrases, Verb Phrases). It is most effective when extracting specific spans of text for phrase-level classification **(Stack Overflow)**.\n",
        "* **Dependency Parsing**: Connects words according to binary head-dependent relations **(Stack Overflow)**. In the sentence *\"The developer refactored the code,\"* \"refactored\" is the head, and \"developer\" is the subject **(arXiv)**.\n",
        "\n",
        "### üè≠ Appraise Dependency Parsing in Enterprise Workflows\n",
        "Research from **SAP** evaluates dependency-based knowledge graph construction as a cost-effective, \"industrial-grade\" alternative to LLM-driven extraction. The **DependencyExtractor** methodology follows five technical stages **(arXiv)**:\n",
        "\n",
        "1. **Noun Phrase Extraction and Cleaning**: Identifying and normalizing the primary entities.\n",
        "2. **Verb Processing**: Extracting the relation (the action) that links entities.\n",
        "3. **Subject/Object Identification**: Mapping the directionality of the relationship.\n",
        "4. **Special Pattern Recognition**: Handling technical syntax or domain-specific language structures.\n",
        "5. **Triple Formation**: Constructing \"subject-relation-object\" sets for materialization in the knowledge graph.\n",
        "\n",
        "---\n",
        "\n",
        "## 4. The Problem Space: Ambiguity, Noise, and Domain Jargon\n",
        "The technical challenges of raw text remain a primary obstacle to enterprise-scale AI. Three primary hurdles define this space:\n",
        "\n",
        "* **Semantic Ambiguity**: Word Sense Disambiguation is required for terms like \"bat\" or \"right\" (direction vs. legal claim). Without context, models may incorrectly correlate unrelated concepts **(AWS)**.\n",
        "* **Linguistic Noise**: Digital communication involves abbreviations (e.g., \"smth\") and lengthened words (e.g., \"hellooo\"). Text Normalization is required to convert these into a **Canonical Representation** (e.g., \"something\" or \"hello\") **(Scale Events)**.\n",
        "* **Domain-Specific Jargon**: General NLP rules often fail when processing scientific documents containing mathematical symbols and equations, or technical logs like **SAP Custom Code Migration (CCM)** logs **(Scale Events, arXiv)**.\n",
        "\n",
        "---\n",
        "\n",
        "## 5. Comparative Industry Analysis: Healthcare vs. Finance\n",
        "Domain adaptation is a strategic necessity; general NLP rules often fail when applied to specialized clinical or financial market data **(arXiv, Shaip)**.\n",
        "\n",
        "| Sector | NLP Focus Area | Strategic Objectives & Requirements |\n",
        "| :--- | :--- | :--- |\n",
        "| **Healthcare** | Clinical notes, Electronic Health Records (EHR), and physician dictation **(AWS, Shaip)**. | **Objectives**: Predictive diagnostics through the extraction of patterns in clinical history. <br> **Mandate**: Critical \"So What?\" layer involves sensitive data redaction for HIPAA and privacy compliance. |\n",
        "| **Finance** | Earning reports, risk flagging in contracts, and SAP Custom Code Migration (CCM) logs **(arXiv, Shaip)**. | **Objectives**: Alpha generation and risk mitigation. <br> **Mandate**: Mapping complex transactional dependencies and identifying risk flags within multi-system procurement modules. |\n",
        "\n",
        "---\n",
        "\n",
        "## 6. Advanced Architectures: GraphRAG and Edge Deployment\n",
        "Enterprise environments are shifting toward **GraphRAG** to solve the limitations of traditional RAG in multi-hop reasoning. While standard RAG retrieves isolated snippets, GraphRAG utilizes a structured knowledge graph to enable traversal-based querying **(arXiv)**.\n",
        "\n",
        "### üìà Evaluate Scalable GraphRAG\n",
        "The SAP \"Multi-model KG Construction Pipeline\" contrasts two paths **(arXiv)**:\n",
        "* **High-Quality Path**: Uses LLMs (GPT-4o) for extraction; accurate but slow and computationally expensive.\n",
        "* **Lightweight Path**: Uses a dependency-parser-based builder. It maintains **94% of the performance** of the LLM path in context precision while using an architectural stack involving **iGraph** (in-memory graph store) and **Milvus** (Vector DB). Mechanisms like **One-hop traversal** and **Reciprocal Rank Fusion (RRF)** ensure low-latency, high-recall query performance.\n",
        "\n",
        "### üì± Optimization for the Edge\n",
        "Deploying NLP on resource-constrained mobile or IoT devices requires three core optimization techniques **(ICMLAS 2025)**:\n",
        "\n",
        "1. **Pruning**: Eliminating redundant parameters to lower memory demands.\n",
        "2. **Quantization**: Reducing precision (e.g., 32-bit to 8-bit integers) to decrease computational overhead.\n",
        "3. **Knowledge Distillation**: Training \"student\" models to replicate \"teacher\" model behavior.\n",
        "\n",
        "#### **Quantified Impact of Optimization (ICMLAS 2025):**\n",
        "* **Model Size**: Pruning can reduce model size by **60%** (e.g., from 500 MB to 200 MB).\n",
        "* **Inference Speed**: Pruning can decrease inference time to **125 ms**.\n",
        "* **Memory Efficiency**: Quantization reduces the memory footprint by **50%** while maintaining **91.2% accuracy**.\n",
        "* **Training Time**: Knowledge distillation can halve training duration (from 30 hours to 15 hours).\n",
        "\n",
        "---\n",
        "\n",
        "## 7. Future Trajectories and Ethical Considerations\n",
        "The NLP market is accelerating on a trajectory toward a projected value of **$39.37 billion by 2025 (Shaip)**. Industrial analysis indicates several defining trends:\n",
        "\n",
        "* **Real-Time Translation**: Breaking language barriers with up to **98% accuracy** in spoken and written formats **(Shaip)**.\n",
        "* **Emotional Intelligence**: Moving beyond sentiment to detect complex states like frustration, joy, or sarcasm in customer interactions **(Shaip)**.\n",
        "* **Multilingual Support**: Google‚Äôs **Universal Speech Model (USM)** aims to cover 1,000 languages, currently supporting over **400 languages (Shaip)**.\n",
        "* **Market Dominance**: North America currently leads the global market with a **30.7% revenue share (Shaip)**.\n",
        "\n",
        "> **The Ethical Mandate**: As NLP matures, \"Ethical AI\" is becoming a priority. Organizations face rising mandates to disclose training data sources to mitigate hallucinations and algorithmic biases in sensitive areas like hiring or lending **(Shaip)**.\n",
        "\n",
        "### üéØ Conclusion\n",
        "Natural Language Processing has evolved from basic text cleanup into the essential engine for the **2025 \"AI Era,\"** enabling a sophisticated, structured understanding of the human world."
      ],
      "metadata": {
        "id": "QUg6qZ0RebhI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Reading in Text Data From Various File Formats**\n",
        "\n",
        "\n",
        "**1. Reading Data from a CSV File:**"
      ],
      "metadata": {
        "id": "HIvoBXu8jmRQ"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d370e0e7",
        "outputId": "a05691b3-bbde-43d6-e5ac-ce9400603559"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Create a dummy DataFrame with a 'Review Text' column\n",
        "dummy_data = {\n",
        "    'Review Text': [\n",
        "        'This product is amazing! I love it.',\n",
        "        'It was okay, but could be better.',\n",
        "        'Absolutely terrible, a complete waste of money.',\n",
        "        'Good value for the price.',\n",
        "        'Not bad, but not great either.'\n",
        "    ]\n",
        "}\n",
        "dummy_df = pd.DataFrame(dummy_data)\n",
        "\n",
        "# Save the dummy DataFrame to 'reviews.csv'\n",
        "dummy_df.to_csv('reviews.csv', index=False)\n",
        "\n",
        "print(\"Dummy 'reviews.csv' created successfully.\")"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dummy 'reviews.csv' created successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2. Reading Data from a Plain Text File:**"
      ],
      "metadata": {
        "id": "xxTiMBjEkEBD"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aff65052",
        "outputId": "4ccc9acb-4b31-4f40-f0db-3b5dbc163d85"
      },
      "source": [
        "# Create a dummy comments.txt file\n",
        "file_path = \"comments.txt\"\n",
        "with open(file_path, \"w\") as file:\n",
        "    file.write(\"Loved the service!\\n\")\n",
        "    file.write(\"Would not recommend.\\n\")\n",
        "    file.write(\"Amazing experience overall.\\n\")\n",
        "\n",
        "print(\"Dummy 'comments.txt' created successfully.\")"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dummy 'comments.txt' created successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Read the text file\n",
        "file_path = \"comments.txt\"\n",
        "with open(file_path, \"r\") as file:\n",
        "    lines = file.readlines()\n",
        "\n",
        "# Print each line\n",
        "for line in lines:\n",
        "    print(line.strip())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0McvJuEaksXh",
        "outputId": "47b8ac86-e124-4a48-cdd9-d8dbc91e4a0e"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loved the service!\n",
            "Would not recommend.\n",
            "Amazing experience overall.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3. Reading Data from a JSON File:**"
      ],
      "metadata": {
        "id": "H5yaasIZkV7B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "[\n",
        "    {\"id\": 1, \"text\": \"This is amazing!\"},\n",
        "    {\"id\": 2, \"text\": \"Not satisfied with the product.\"},\n",
        "    {\"id\": 3, \"text\": \"Would buy again!\"}\n",
        "]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y2bxRxdMkXmV",
        "outputId": "a281c2b4-152f-4d39-f5a1-1948cc3e6e03"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'id': 1, 'text': 'This is amazing!'},\n",
              " {'id': 2, 'text': 'Not satisfied with the product.'},\n",
              " {'id': 3, 'text': 'Would buy again!'}]"
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "45db93bf",
        "outputId": "40c3b3dd-9ed1-4b71-e38c-bf94a891ccf2"
      },
      "source": [
        "import json\n",
        "\n",
        "json_content = [\n",
        "    {\"id\": 1, \"text\": \"This is amazing!\"},\n",
        "    {\"id\": 2, \"text\": \"Not satisfied with the product.\"},\n",
        "    {\"id\": 3, \"text\": \"Would buy again!\"}\n",
        "]\n",
        "\n",
        "file_path = \"data.json\"\n",
        "with open(file_path, \"w\") as file:\n",
        "    json.dump(json_content, file, indent=4)\n",
        "\n",
        "print(\"Dummy 'data.json' created successfully.\")"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dummy 'data.json' created successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "# Read the JSON file\n",
        "file_path = \"data.json\"\n",
        "with open(file_path, \"r\") as file:\n",
        "    data = json.load(file)\n",
        "\n",
        "# Extract and print text values\n",
        "for record in data:\n",
        "    print(record[\"text\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z8I7sRN2kZ5L",
        "outputId": "6f581fcf-ada0-48db-ec1c-1fbfb125cae1"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "This is amazing!\n",
            "Not satisfied with the product.\n",
            "Would buy again!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Reading a .txt File:\n",
        "\n",
        "\n",
        "# Create a dummy data.txt file\n",
        "file_path = \"data.txt\"\n",
        "with open(file_path, \"w\") as file:\n",
        "    file.write(\"This is the first line.\\n\")\n",
        "    file.write(\"This is the second line.\\n\")\n",
        "    file.write(\"And this is the third line.\")\n",
        "\n",
        "print(\"Dummy 'data.txt' created successfully.\")\n",
        "\n",
        "# Open the text file\n",
        "file_path = \"data.txt\"\n",
        "with open(file_path, \"r\") as file:\n",
        "    # Read lines from the file\n",
        "    lines = file.readlines()\n",
        "\n",
        "# Print each line after stripping whitespace\n",
        "for line in lines:\n",
        "    print(line.strip())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "26fV4QA2k7Xk",
        "outputId": "c084febd-51a7-4b4a-83ca-65f1a9dd98b9"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dummy 'data.txt' created successfully.\n",
            "This is the first line.\n",
            "This is the second line.\n",
            "And this is the third line.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Common Preprocessing Techniques**\n",
        "\n",
        "**1. Lowercasing:**"
      ],
      "metadata": {
        "id": "n1zDBp6ZgB9e"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kaOz6HAXeTPP",
        "outputId": "682470cf-d7a2-4c12-9571-fb1a78c43558"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "natural language processing is amazing!\n"
          ]
        }
      ],
      "source": [
        "# Sample text\n",
        "text = \"Natural Language Processing is AMAZING!\"\n",
        "\n",
        "# Convert to lowercase\n",
        "cleaned_text = text.lower()\n",
        "print(cleaned_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2. Punctuation Removal:**"
      ],
      "metadata": {
        "id": "Xlz51R1ggOzK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "# Sample text\n",
        "text = \"Hello, world! Welcome to NLP.\"\n",
        "\n",
        "# Remove punctuation using regex\n",
        "cleaned_text = re.sub(r\"[^\\w\\s']\", \"\", text)\n",
        "print(cleaned_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7KsEcHaYgQzU",
        "outputId": "b1d0beac-3891-4321-cc5f-11e2b1b40b17"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello world Welcome to NLP\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* The **re.sub()** function is used to replace punctuation with an empty string.\n",
        "\n",
        "\n",
        "\n",
        "* The regex pattern **[^\\w\\s']** matches any character that is not a word **(\\w)** or a space **(\\s)**."
      ],
      "metadata": {
        "id": "dUSdpo1ngbCI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3. Removing Extra Whitespaces:**\n",
        "\n",
        "* The **.split()** method splits the text into words by whitespace.\n",
        "* The **\" \".join()** method reassembles the words into a single string, removing extra spaces.\n"
      ],
      "metadata": {
        "id": "NxlaWkWjge5N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Sample text\n",
        "text = \"  This   is   a   sentence   with   extra   spaces.   \"\n",
        "\n",
        "# Remove extra whitespaces between the words\n",
        "cleaned_text = \" \".join(text.split())\n",
        "\n",
        "print(cleaned_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PI6b8wCkgkam",
        "outputId": "89ee48d5-8d1c-4a6c-b2d9-92714ccf1f9e"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "This is a sentence with extra spaces.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**4. Removing Numbers:**\n",
        "\n",
        "* The regex pattern **\\d+** matches one or more digits in the text.\n",
        "* **re.sub()** replaces the matched digits with an empty string."
      ],
      "metadata": {
        "id": "_MGD6cahhA4D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "# Sample text\n",
        "text = \"The price is 100 dollars.\"\n",
        "\n",
        "# Remove numbers using regex\n",
        "cleaned_text = re.sub(r\"\\d+\", \"\", text)\n",
        "print(cleaned_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FMOSRpXAhCmi",
        "outputId": "a8e5b44b-b732-4fb0-dd65-cac35a88e1b2"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The price is  dollars.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**5. Handling Case-Specific Words:**\n",
        "\n",
        "* The regex pattern **\\b(and|or|but)\\b** matches whole words \"and\", \"or\", and \"but\".\n",
        "\n",
        "* After removing the words, we use the **split()** and **join()** methods to clean up extra spaces."
      ],
      "metadata": {
        "id": "Qu9rCehGhfHh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Sample text\n",
        "text = \"Stop words like 'and', 'or', and 'but' can be removed.\"\n",
        "\n",
        "# Replace specific words\n",
        "cleaned_text = re.sub(r\"\\b(and|or|but)\\b\", \"\", text)\n",
        "cleaned_text = \" \".join(cleaned_text.split())\n",
        "print(cleaned_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IObwBF1Yhr4w",
        "outputId": "9ef21495-3fb6-419a-9182-704fa96734af"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stop words like '', '', '' can be removed.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Challenge 1: Lowercasing and Punctuation Removal**\n",
        "\n",
        "\n",
        "**Task:** Write a function clean_text() that takes a string and returns it in lowercase with punctuation removed."
      ],
      "metadata": {
        "id": "b0UVDf-mh7K1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "def clean_text(text):\n",
        "    \"\"\"Clean text by lowercasing and removing punctuation.\"\"\"\n",
        "    # Convert to lowercase\n",
        "    text = text.lower()\n",
        "    # Remove punctuation\n",
        "    text = re.sub(r\"[^\\w\\s']\", \"\", text)\n",
        "    return text\n",
        "\n",
        "# Test the function\n",
        "sample_text = \"Hello, NLP World!\"\n",
        "print(clean_text(sample_text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u8ACkq2mh-Q1",
        "outputId": "28a1dc01-7be3-403a-d2ad-1debb86ba7f1"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hello nlp world\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Challenge 2: Removing Numbers and Extra Whitespaces**\n",
        "\n",
        "**Task:** Write a function clean_text_numbers_spaces() that removes numbers and extra whitespaces from a string."
      ],
      "metadata": {
        "id": "xg_KFZzkiC_K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "def clean_text_numbers_spaces(text):\n",
        "    \"\"\"Clean text by removing numbers and extra spaces.\"\"\"\n",
        "    # Remove numbers\n",
        "    text = re.sub(r\"\\d+\", \"\", text)\n",
        "    # Remove extra spaces\n",
        "    text = \" \".join(text.split())\n",
        "    return text\n",
        "\n",
        "# Test the function\n",
        "sample_text = \"This 123 text   has 456 extra spaces and 789 numbers.\"\n",
        "print(clean_text_numbers_spaces(sample_text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qt8G3Y7DiGwP",
        "outputId": "e257ce77-6d21-4d97-9095-b36553b61b8c"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "This text has extra spaces and numbers.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Bag of Words: Implementation"
      ],
      "metadata": {
        "id": "Jvrao_eIibbJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# Sample data\n",
        "documents = [\"I love programming.\", \"Programming is fun.\"]\n",
        "\n",
        "# Initialize the CountVectorizer\n",
        "vectorizer = CountVectorizer()\n",
        "\n",
        "# Fit and transform the documents\n",
        "X = vectorizer.fit_transform(documents)\n",
        "\n",
        "# Convert the result to an array and print\n",
        "print(\"Vocabulary:\", vectorizer.get_feature_names_out())  # Get the vocabulary\n",
        "print(\"BoW Matrix:\\n\", X.toarray())  # Display the document-term matrix"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I6QL-VSwic-n",
        "outputId": "523e85f2-13fc-491d-9aaf-b3fb2613adaf"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary: ['fun' 'is' 'love' 'programming']\n",
            "BoW Matrix:\n",
            " [[0 0 1 1]\n",
            " [1 1 0 1]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TF-IDF: Implementation"
      ],
      "metadata": {
        "id": "3nkVh3TLiixS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Sample data\n",
        "documents = [\"I love programming.\", \"Programming is fun.\"]\n",
        "\n",
        "# Initialize the TfidfVectorizer\n",
        "vectorizer = TfidfVectorizer()\n",
        "\n",
        "# Fit and transform the documents\n",
        "X = vectorizer.fit_transform(documents)\n",
        "\n",
        "# Convert the result to an array and print\n",
        "print(\"Vocabulary:\", vectorizer.get_feature_names_out())  # Get the vocabulary\n",
        "print(\"TF-IDF Matrix:\\n\", X.toarray())  # Display the TF-IDF matrix"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9t5pjT18ijrk",
        "outputId": "282be90f-8030-4da2-97de-4daa9284c629"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary: ['fun' 'is' 'love' 'programming']\n",
            "TF-IDF Matrix:\n",
            " [[0.         0.         0.81480247 0.57973867]\n",
            " [0.6316672  0.6316672  0.         0.44943642]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# üìä Bag of Words vs. TF-IDF: A Technical Comparison\n",
        "\n",
        "In modern Natural Language Processing (NLP), choosing the right vectorization strategy is critical for model performance. This table provides a detailed comparative analysis between the **Bag of Words (BoW)** model and **Term Frequency-Inverse Document Frequency (TF-IDF)**.\n",
        "\n",
        "---\n",
        "\n",
        "| Aspect | Bag of Words (BoW) | TF-IDF |\n",
        "| :--- | :--- | :--- |\n",
        "| **Word Frequency** | Utilizes raw word counts for vector representation. | Employs term frequency adjusted by the inverse document frequency. |\n",
        "| **Order of Words** | Neglects word order and spatial relationships within the text. | Neglects word order and spatial relationships within the text. |\n",
        "| **Focus** | Concentrates on the raw occurrence of words within an individual document. | Concentrates on the statistical significance of words within a broader corpus. |\n",
        "| **Handling Common Words** | Assigns equal weight to all words, including non-discriminatory common terms like \"the\". | Down-weights common \"noise\" words and highlights rare, semantically significant words. |\n",
        "| **Use Case** | Optimal for fundamental text analysis tasks where raw frequency is the primary metric. | Superior for tasks requiring sophisticated importance ranking, such as search engine indexing. |\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "### üí° Key Takeaway\n",
        "While **Bag of Words** is efficient for simple classification, **TF-IDF** provides a more nuanced understanding of \"meaning\" by filtering out the linguistic noise common in large datasets."
      ],
      "metadata": {
        "id": "xwHQUrXCiy58"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Challenge 1: Implement Bag of Words**"
      ],
      "metadata": {
        "id": "rSDcaUKWi0wM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# Sample data\n",
        "documents = [\"Machine learning is fun.\", \"Deep learning is a subset of machine learning.\"]\n",
        "# Initialize the CountVectorizer\n",
        "vectorizer = CountVectorizer()\n",
        "\n",
        "# Fit and transform the documents\n",
        "X = vectorizer.fit_transform(documents)\n",
        "\n",
        "# Display the vocabulary and the BoW matrix\n",
        "print(\"Vocabulary:\", vectorizer.get_feature_names_out())  # Get the vocabulary\n",
        "# BoW Matrix:\n",
        "print(X.toarray())  # Display the document-term matrix"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h2dqb-CTjHtv",
        "outputId": "a5a5c6d3-6553-43ca-dff9-29791ba09556"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary: ['deep' 'fun' 'is' 'learning' 'machine' 'of' 'subset']\n",
            "[[0 1 1 1 1 0 0]\n",
            " [1 0 1 2 1 1 1]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Challenge 2: Implement TF-IDF**"
      ],
      "metadata": {
        "id": "QT4c0tiPjK1v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Sample data\n",
        "documents = [\"Machine learning is fun.\", \"Deep learning is a subset of machine learning.\"]\n",
        "# Initialize the TfidfVectorizer\n",
        "vectorizer = TfidfVectorizer()\n",
        "\n",
        "# Fit and transform the documents\n",
        "X = vectorizer.fit_transform(documents)\n",
        "\n",
        "# Display the vocabulary and the TF-IDF matrix\n",
        "print(\"Vocabulary:\", vectorizer.get_feature_names_out())  # Get the vocabulary\n",
        "# TF-IDF Matrix:\n",
        "print(X.toarray())  # Display the TF-IDF matrix"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eOCzgrWzjNdv",
        "outputId": "657dd429-764c-4345-8cca-c7690f61ff34"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary: ['deep' 'fun' 'is' 'learning' 'machine' 'of' 'subset']\n",
            "[[0.         0.63009934 0.44832087 0.44832087 0.44832087 0.\n",
            "  0.        ]\n",
            " [0.40697968 0.         0.2895694  0.57913879 0.2895694  0.40697968\n",
            "  0.40697968]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Challenge 3: Comparing Bag of Words and TF-IDF**"
      ],
      "metadata": {
        "id": "ufJ1bzvujSlU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "\n",
        "# Sample data\n",
        "documents = [\"Data science is exciting!\", \"Data science requires programming knowledge.\", \"Programming is essential for data science.\"]\n",
        "\n",
        "# Initialize the CountVectorizer and TfidfVectorizer\n",
        "vectorizer_bow = CountVectorizer()\n",
        "vectorizer_tfidf = TfidfVectorizer()\n",
        "\n",
        "# Fit and transform the documents using both vectorizers\n",
        "X_bow = vectorizer_bow.fit_transform(documents)\n",
        "X_tfidf = vectorizer_tfidf.fit_transform(documents)\n",
        "\n",
        "# Display the results\n",
        "# BoW Matrix:\n",
        "print(X_bow.toarray())\n",
        "# TF-IDF Matrix:\n",
        "print(X_tfidf.toarray())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cye6UOZ-jTvj",
        "outputId": "2c611efb-3b4b-4f72-fa4d-b18b4e8ffc07"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[1 0 1 0 1 0 0 0 1]\n",
            " [1 0 0 0 0 1 1 1 1]\n",
            " [1 1 0 1 1 0 1 0 1]]\n",
            "[[0.39148397 0.         0.66283998 0.         0.50410689 0.\n",
            "  0.         0.         0.39148397]\n",
            " [0.32630952 0.         0.         0.         0.         0.55249005\n",
            "  0.42018292 0.55249005 0.32630952]\n",
            " [0.30083189 0.50935267 0.         0.50935267 0.38737583 0.\n",
            "  0.38737583 0.         0.30083189]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Hands-on Practice**"
      ],
      "metadata": {
        "id": "1QUXaKMlxdbI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Challenge 1: Reading Text Data from Multiple File Formats**\n",
        "\n",
        "**Task:**\n",
        "\n",
        "Read in text data from three file formats:\n",
        "* A CSV file that contains a column with text data. Extract the text from the column named \"Review Text\".\n",
        "* A TXT file with multiple lines. Read the lines and print each one.\n",
        "* A JSON file where the \"text\" key contains the text data. Extract and print the text for each entry."
      ],
      "metadata": {
        "id": "AY0gL9xzxmhK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import json\n",
        "\n",
        "# Create a dummy sample.txt file\n",
        "file_path_txt = \"sample.txt\"\n",
        "with open(file_path_txt, \"w\") as file:\n",
        "    file.write(\"This is the first line of sample text.\\n\")\n",
        "    file.write(\"This is the second line.\\n\")\n",
        "    file.write(\"And this is the third line.\")\n",
        "\n",
        "print(\"Dummy 'sample.txt' created successfully.\")\n",
        "\n",
        "# Create a dummy sample.json file\n",
        "json_content = [\n",
        "    {\"id\": 1, \"text\": \"This is JSON text 1.\"},\n",
        "    {\"id\": 2, \"text\": \"This is JSON text 2.\"}\n",
        "]\n",
        "file_path_json_create = \"sample.json\"\n",
        "with open(file_path_json_create, \"w\") as file:\n",
        "    json.dump(json_content, file, indent=4)\n",
        "print(\"Dummy 'sample.json' created successfully.\")\n",
        "\n",
        "# Read in the CSV file and extract the \"Review Text\" column\n",
        "file_path_csv = \"reviews.csv\"\n",
        "data_csv = pd.read_csv(file_path_csv)  # Read the CSV file\n",
        "reviews = data_csv[\"Review Text\"]  # Extract the \"Review Text\" column\n",
        "print(\"\\n--- Reading from reviews.csv ---\")\n",
        "print(reviews)\n",
        "\n",
        "# Read in the TXT file and print each line\n",
        "print(\"\\n--- Reading from sample.txt ---\")\n",
        "with open(file_path_txt, \"r\") as txt_file:\n",
        "    lines = txt_file.readlines()  # Use readlines() to get all lines\n",
        "    for line in lines:\n",
        "        print(line.strip())\n",
        "\n",
        "# Read in the JSON file and extract the text data from the \"text\" key\n",
        "print(\"\\n--- Reading from sample.json ---\")\n",
        "file_path_json_read = \"sample.json\"\n",
        "with open(file_path_json_read, \"r\") as json_file:\n",
        "    data_json = json.load(json_file)\n",
        "    for entry in data_json:\n",
        "        print(entry[\"text\"]) # Access the \"text\" key"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K2wvsLGtxgDq",
        "outputId": "3f0fde08-1a81-4522-ca59-51bb4dc5fee0"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dummy 'sample.txt' created successfully.\n",
            "Dummy 'sample.json' created successfully.\n",
            "\n",
            "--- Reading from reviews.csv ---\n",
            "0                This product is amazing! I love it.\n",
            "1                  It was okay, but could be better.\n",
            "2    Absolutely terrible, a complete waste of money.\n",
            "3                          Good value for the price.\n",
            "4                     Not bad, but not great either.\n",
            "Name: Review Text, dtype: object\n",
            "\n",
            "--- Reading from sample.txt ---\n",
            "This is the first line of sample text.\n",
            "This is the second line.\n",
            "And this is the third line.\n",
            "\n",
            "--- Reading from sample.json ---\n",
            "This is JSON text 1.\n",
            "This is JSON text 2.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dfa357c5",
        "outputId": "046c3d04-5f89-468d-8237-a0fe16b6c852"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Create a dummy DataFrame with a 'Review Text' column\n",
        "dummy_data = {\n",
        "    'Review Text': [\n",
        "        'This product is amazing! I love it.',\n",
        "        'It was okay, but could be better.',\n",
        "        'Absolutely terrible, a complete waste of money.',\n",
        "        'Good value for the price.',\n",
        "        'Not bad, but not great either.'\n",
        "    ]\n",
        "}\n",
        "dummy_df = pd.DataFrame(dummy_data);\n",
        "\n",
        "# Save the dummy DataFrame to 'reviews.csv'\n",
        "dummy_df.to_csv('reviews.csv', index=False);\n",
        "\n",
        "print(\"Dummy 'reviews.csv' created successfully.\")"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dummy 'reviews.csv' created successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Challenge 2: String Operations for Text Cleaning**\n",
        "**Task:**\n",
        "\n",
        "Given a sample text, clean it by:\n",
        "\n",
        "* Removing punctuation using the re library.\n",
        "* Converting to lowercase.\n",
        "* Stripping extra spaces."
      ],
      "metadata": {
        "id": "51_apjFvzKVN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "sample_text = \"  Hello, World!! NLP is amazing, isn't it?  \"\n",
        "\n",
        "# Remove punctuation using regex\n",
        "cleaned_text = re.sub(r\"[^\\w\\s]\", \"\", sample_text)\n",
        "\n",
        "# Convert to lowercase\n",
        "text_lowercase = cleaned_text.lower()\n",
        "\n",
        "# Strip extra spaces\n",
        "final_text = text_lowercase.strip()\n",
        "print(final_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TBD3X_qWzSLv",
        "outputId": "d8a030b5-0a17-4ec6-c51f-aa09a99c6acc"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hello world nlp is amazing isnt it\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Challenge 3: Tokenization and Stop-Words Removal**\n",
        "**Task:**\n",
        "Tokenize the given text and remove stop words using NLTK."
      ],
      "metadata": {
        "id": "BSp8DsudzeLp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "nltk.download(\"stopwords\")\n",
        "nltk.download(\"punkt\")\n",
        "nltk.download(\"punkt_tab\") # Add this line to download the missing resource\n",
        "\n",
        "sample_text = \"NLP is a fascinating field of study with diverse applications.\"\n",
        "\n",
        "# Tokenize the text\n",
        "tokens = word_tokenize(sample_text)\n",
        "\n",
        "# Remove stop words\n",
        "stop_words = set(stopwords.words(\"english\"))\n",
        "filtered_tokens = [word for word in tokens if word.lower() not in stop_words]\n",
        "print(filtered_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jibUhWjazhrh",
        "outputId": "76910fde-b126-4a7d-abac-2bd040cce475"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['NLP', 'fascinating', 'field', 'study', 'diverse', 'applications', '.']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Challenge 4: Stemming and Lemmatization**\n",
        "\n",
        "**Task:**\n",
        "\n",
        "Perform stemming and lemmatization on a list of words. Even though we know now that the accuracy of lemmatization may be improved if we specify the part of speech for each lemmatized word, in this challenge we want to ask you to go with the default lemmatizer configuration."
      ],
      "metadata": {
        "id": "x05MhMM_0GPC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "nltk.download(\"wordnet\")\n",
        "\n",
        "words = [\"running\", \"flies\", \"better\", \"easily\", \"happiest\"]\n",
        "\n",
        "# Stem the words\n",
        "stemmer = PorterStemmer()\n",
        "stems = [stemmer.stem(word) for word in words]\n",
        "\n",
        "# Lemmatize the words\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "lemmas = [lemmatizer.lemmatize(word) for word in words]\n",
        "print(\"Stems:\", stems)\n",
        "print(\"Lemmas:\", lemmas)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Evp9ROSm0M6o",
        "outputId": "c224fa38-6ac8-4cdc-bc6b-fdfcae578f52"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stems: ['run', 'fli', 'better', 'easili', 'happiest']\n",
            "Lemmas: ['running', 'fly', 'better', 'easily', 'happiest']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "If you are interested in knowing how to lemmatize and achieve the best accuracy, check out the below example that performs lemmatization for each part of speech separately:"
      ],
      "metadata": {
        "id": "Yxj0F3LS0WLa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "from nltk.corpus import wordnet\n",
        "\n",
        "nltk.download('wordnet')\n",
        "nltk.download('averaged_perceptron_tagger')  # Needed for POS tagging\n",
        "nltk.download('averaged_perceptron_tagger_eng') # Download the specific English tagger\n",
        "\n",
        "words = [\"running\", \"flies\", \"better\", \"easily\", \"happiest\"]\n",
        "\n",
        "# Stem the words\n",
        "stemmer = PorterStemmer()\n",
        "stems = [stemmer.stem(word) for word in words]\n",
        "\n",
        "# Lemmatize with POS tagging\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# Function to map NLTK POS tags to WordNet POS tags\n",
        "def get_wordnet_pos(treebank_tag):\n",
        "    if treebank_tag.startswith('J'):\n",
        "        return wordnet.ADJ\n",
        "    elif treebank_tag.startswith('V'):\n",
        "        return wordnet.VERB\n",
        "    elif treebank_tag.startswith('N'):\n",
        "        return wordnet.NOUN\n",
        "    elif treebank_tag.startswith('R'):\n",
        "        return wordnet.ADV\n",
        "    else:\n",
        "        return wordnet.NOUN  # Default to noun\n",
        "\n",
        "# Get POS tags for each word\n",
        "pos_tags = nltk.pos_tag(words)\n",
        "\n",
        "# Lemmatize with POS tags\n",
        "lemmas = []\n",
        "for word, tag in pos_tags:\n",
        "    wn_pos = get_wordnet_pos(tag)\n",
        "    lemmas.append(lemmatizer.lemmatize(word, pos=wn_pos))\n",
        "\n",
        "print(\"Stems:\", stems)\n",
        "print(\"Lemmas:\", lemmas)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "obgW1YVS0XCR",
        "outputId": "1dfffd61-6d38-411d-9ee9-70f8bce312a8"
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger_eng.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stems: ['run', 'fli', 'better', 'easili', 'happiest']\n",
            "Lemmas: ['run', 'fly', 'well', 'easily', 'happiest']\n"
          ]
        }
      ]
    }
  ]
}